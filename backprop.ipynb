{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpOoxE2d-mOY"
      },
      "source": [
        "## Ejercicio teórico\n",
        "\n",
        "Sea una red neuronal de dos capas, la primera de 3 neuronas y la segunda de 1 con los parámetros inicializados con los siguientes valores:\n",
        "$$\n",
        "w^{(1)} =\n",
        "\\begin{pmatrix}\n",
        "0.1 & -0.5 \\\\\n",
        "-0.3 & -0.9 \\\\\n",
        "0.8 & 0.02\n",
        "\\end{pmatrix},\n",
        "b^{(1)} = \\begin{pmatrix}\n",
        "0.1 \\\\\n",
        "0.5 \\\\\n",
        "0.8\n",
        "\\end{pmatrix},\n",
        "w^{(2)} =\n",
        "\\begin{pmatrix}\n",
        "-0.4 & 0.2 & -0.5\n",
        "\\end{pmatrix},\n",
        "b^{(2)} = 0.7\n",
        "$$\n",
        "\n",
        "y donde cada capa calcula su salida vía\n",
        "\n",
        "$$\n",
        "y^{(i)} = \\sigma (w^{(i)} \\cdot x^{(i)}+b^{(i)})\n",
        "$$\n",
        "\n",
        "donde $\\sigma (z) = \\frac{1}{1+e^{-z}}$ es la función sigmoidea .\n",
        "\n",
        "\\\\\n",
        "Dada la observación $x=\\begin{pmatrix}\n",
        "1.8 \\\\\n",
        "-3.4\n",
        "\\end{pmatrix}$, $y=5$ y la función de costo $J(\\theta)=\\frac{1}{2}(\\hat{y}_\\theta-y)^2$, calcular las derivadas de J respecto de cada parámetro $w^{(1)}$, $w^{(2)}$, $b^{(1)}$, $b^{(2)}$.\n",
        "\n",
        "*Nota: Con una sigmoidea a la salida jamás va a poder estimar el 5 \"pedido\", pero eso no afecta al mecanismo de backpropagation!*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Layer(object):\n",
        "  def __init__(self, n_in, n_out, non_linearity_class, optimizer_factory, rng, w_init=None, b_init=None):\n",
        "    self.activation = non_linearity_class()\n",
        "    self.optim = optimizer_factory()\n",
        "    #self.w = rng.standard_normal(size=(n_out, n_in))  * 0.1 # W shape is (n_out,n_in)\n",
        "    #self.b = rng.uniform(size=(n_out, 1))                   # b shape is (n_out, 1)\n",
        "    # Valores de peso y bias precalculados\n",
        "    self.w = w_init if w_init is not None else rng.standard_normal(size=(n_out, n_in)) * 0.1\n",
        "    self.b = b_init if b_init is not None else rng.uniform(size=(n_out, 1))\n",
        "    self.last_output = None\n",
        "    self.last_input = None\n",
        "\n",
        "  def forward(self, X):\n",
        "    self.last_input = X\n",
        "    z = self.w @ X + self.b\n",
        "    self.last_output = self.activation.f(z)\n",
        "    print(\"--------------------------\")\n",
        "    print(\"Forward por capa z:\")\n",
        "    print(z)\n",
        "    print(\"Despues de la f de activación:\")\n",
        "    print(self.last_output)\n",
        "    print(\"--------------------------\")\n",
        "    return self.last_output\n",
        "\n",
        "  def backwards(self, dY):\n",
        "    dz = dY * self.activation.df()\n",
        "    dW = dz @ self.last_input.T\n",
        "    db = np.sum(dz, axis=1, keepdims=True)\n",
        "    dX = self.w.T @ dz\n",
        "    self.w, self.b = self.optim.update(self.w, self.b, dW, db)\n",
        "\n",
        "    # Mostrar derivadas parciales\n",
        "    print(\"--------------------------\")\n",
        "    print(\"Derivadas parciales respecto a W:\")\n",
        "    print(dW)\n",
        "    print(\"Derivadas parciales respecto a b:\")\n",
        "    print(\"--------------------------\")\n",
        "    print(db)\n",
        "    return dX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP(object):\n",
        "  def __init__(self, dims, optimizer_factory, non_linearities, input_dim, rng_seed = None, precalc_weights=None, precalc_biases=None):\n",
        "    # check lengths\n",
        "    if len(dims) != len(non_linearities):\n",
        "      raise ValueError(\"dims' and Non_linearities' lengths do not match\")\n",
        "    # initialize RNG\n",
        "    rng = np.random.default_rng(rng_seed)\n",
        "    # construct a list of Layers with matching dimension and non-linear activation function\n",
        "    in_dims = [input_dim] + dims[:-1]\n",
        "    #self.layers = [Layer(n_in, n_out, non_linearity, optimizer_factory, rng)\n",
        "    #                for n_in,n_out,non_linearity in zip(in_dims,dims, non_linearities)]\n",
        "    self.layers = [Layer(n_in, n_out, non_linearity, optimizer_factory, rng, w_init, b_init)\n",
        "                    for n_in, n_out, non_linearity, w_init, b_init \n",
        "                    in zip(in_dims, dims, non_linearities, precalc_weights, precalc_biases)]\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "    # X can be interpreted as the output of a previous layer\n",
        "    prediction = X\n",
        "    # sequentially apply forward pass\n",
        "    for layer in self.layers:\n",
        "      prediction = layer.forward(prediction)\n",
        "    return prediction\n",
        "\n",
        "  def update(self, cost_gradient):\n",
        "    # cost gradient is the cost derivative wrt last layer\n",
        "    dY = cost_gradient\n",
        "    # sequentially apply backwards update, in reversed order\n",
        "    for layer in reversed(self.layers):\n",
        "      dY = layer.backwards(dY)\n",
        "\n",
        "  def __repr__(self):\n",
        "    # super hardcoded\n",
        "    return \"MLP with layer sizes: \"+ \"-\".join(str(layer.b.shape[0]) for layer in self.layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Optimizer(object):\n",
        "  def update(self, W, b, dW, db):\n",
        "    raise NotImplementedError(\"optimizer update rule not implemented\")\n",
        "\n",
        "class VGD(Optimizer):\n",
        "  def __init__(self, learning_rate):\n",
        "    self.lr = learning_rate\n",
        "    \n",
        "  def update(self, W_old, b_old, dW, db):\n",
        "    # vanilla GD: theta_t+1 = theta_t - alpha * gradient\n",
        "    W_new = W_old - self.lr * dW\n",
        "    b_new = b_old - self.lr * db\n",
        "    return W_new, b_new\n",
        "\n",
        "def factory_VGD(lr):\n",
        "  return lambda : VGD(lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NonLinearity(object):\n",
        "  def __init__(self):\n",
        "    self.last_z = None\n",
        "  def f(self, z):\n",
        "    raise NotImplementedError(\"function evaluation not implemented\")\n",
        "  def df(self):\n",
        "    raise NotImplementedError(\"function derivative not implemented\")\n",
        "  \n",
        "class Sigmoid(NonLinearity):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "  def sigma():\n",
        "    return \"1 / (1 + np.exp(-z))\"\n",
        "\n",
        "  def f(self, z):\n",
        "    self.last_z = z\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "  def df(self):\n",
        "    return np.exp(-self.last_z) / (1 + np.exp(-self.last_z))**2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Forward de entrada a dalida\n",
            "--------------------------\n",
            "Forward por capa z:\n",
            "[[1.98 ]\n",
            " [3.02 ]\n",
            " [2.172]]\n",
            "Despues de la f de activación:\n",
            "[[0.87868116]\n",
            " [0.95346953]\n",
            " [0.89770677]]\n",
            "--------------------------\n",
            "--------------------------\n",
            "Forward por capa z:\n",
            "[[0.09036805]]\n",
            "Despues de la f de activación:\n",
            "[[0.52257665]]\n",
            "--------------------------\n",
            "--------------------------\n",
            "Salida:  [[0.52257665]]\n",
            "--------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "lr = 0.001\n",
        "rng_seed = 6543\n",
        "\n",
        "# Pesos y sesgos precalculados para cada capa\n",
        "precalc_weights = [\n",
        "    np.array([[0.1, -0.5], [-0.3, -0.9], [0.8, 0.02]]),  # Pesos para la primera capa (3x2)\n",
        "    np.array([[-0.4, 0.2, -0.5]])                        # Pesos para la segunda capa (1x3)\n",
        "]\n",
        "precalc_biases = [\n",
        "    np.array([[0.1], [0.5], [0.8]]),  # Bias para la primera capa (3x1)\n",
        "    np.array([[0.7]])                 # Bias para la segunda capa (1x1)\n",
        "]\n",
        "\n",
        "# Configuración del MLP:\n",
        "dims = [3, 1]  # 3 neuronas en la primera capa, 1 neurona en la segunda\n",
        "input_dim = 2  # 2 entradas (dimensión de entrada)\n",
        "optimizer_factory = lambda: VGD(learning_rate=lr) #No se usa porque el modelo esta pre entrenado\n",
        "non_linearities = [Sigmoid, Sigmoid]  # Activaciones sigmoides para ambas capas\n",
        "\n",
        "# Crear la MLP con pesos y sesgos precalculados\n",
        "mlp = MLP(dims, optimizer_factory, non_linearities, input_dim, precalc_weights=precalc_weights, precalc_biases=precalc_biases)\n",
        "\n",
        "# Input de ejemplo (2 características de entrada)\n",
        "X = np.array([[1.8], [-3.4]])  # Tamaño del input (2x1)\n",
        "\n",
        "# Predecir con el MLP\n",
        "print(\"Forward de entrada a dalida\")\n",
        "output = mlp.predict(X)\n",
        "print(\"--------------------------\")\n",
        "print(\"Salida: \",output)\n",
        "print(\"--------------------------\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backprop de salida a entrada\n",
            "Error cuadratico medio:  [[10.02365992]]\n",
            "--------------------------\n",
            "Derivadas parciales respecto a W:\n",
            "[[-0.98155159 -1.0650957  -1.0028046 ]]\n",
            "Derivadas parciales respecto a b:\n",
            "--------------------------\n",
            "[[-1.11707367]]\n",
            "--------------------------\n",
            "Derivadas parciales respecto a W:\n",
            "[[ 0.0857381  -0.16194975]\n",
            " [-0.01784139  0.0337004 ]\n",
            " [ 0.09232211 -0.1743862 ]]\n",
            "Derivadas parciales respecto a b:\n",
            "--------------------------\n",
            "[[ 0.04763228]\n",
            " [-0.00991188]\n",
            " [ 0.05129006]]\n"
          ]
        }
      ],
      "source": [
        "print(\"Backprop de salida a entrada\")\n",
        "y_true = 5\n",
        "error = 1/2 * (output - y_true)**2\n",
        "print(\"Error cuadratico medio: \", error)\n",
        "mlp.update(output - y_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [],
      "source": [
        "from graphviz import Digraph\n",
        "import numpy as np\n",
        "\n",
        "def matrix_to_text(matrix):\n",
        "    \"\"\"Convierte una matriz numpy en una cadena de texto legible.\"\"\"\n",
        "    text_matrix = \"\\n[\"\n",
        "    for row in matrix:\n",
        "        text_matrix +=  \", \".join(f\"{v:.2f}\" for v in row) + \"\\n\"\n",
        "    text_matrix = text_matrix.rstrip(\"\\n\")\n",
        "    text_matrix += \"]\"\n",
        "    return text_matrix\n",
        "\n",
        "def generate_mlp_graph(mlp):\n",
        "    dot = Digraph()\n",
        "    dot.attr(rankdir='LR', fontname=\"Arial\")  # Configurar la dirección del grafo de izquierda a derecha\n",
        "\n",
        "    # Crear el nodo de entrada X\n",
        "    dot.node(\"X\", \"Input X\", fontname=\"Arial\")\n",
        "\n",
        "    # Iterar a través de las capas de la red\n",
        "    for i, layer in enumerate(mlp.layers):\n",
        "        input_size = layer.w.shape[1]\n",
        "        output_size = layer.w.shape[0]\n",
        "        activation_name = layer.activation.__class__.__name__\n",
        "\n",
        "        # Convertir los pesos y sesgos en un texto legible\n",
        "        weights_text = matrix_to_text(layer.w)\n",
        "        bias_text = matrix_to_text(layer.b)\n",
        "\n",
        "        # Crear nodos para la capa actual con matrices en texto legible\n",
        "        #dot.node(f\"Layer {i+1} input\", f\"Layer {i+1} input ({input_size})\")\n",
        "        dot.node(f\"Layer {i+1} input\", f\"Layer {i+1} input ({input_size})\\nz({i+1})=W*X+b\\nweights {layer.w.shape[0]}x{layer.w.shape[1]}: {weights_text}\\nbias {layer.b.shape[0]}x{layer.b.shape[1]}: {bias_text}\", fontname=\"Arial\")\n",
        "        #dot.node(f\"Layer {i+1} output\", f\"Layer {i+1} output ({output_size})\\nActivation: {activation_name}\\nWeights {layer.w.shape[0]}x{layer.w.shape[1]}: {weights_text}\\nBias {layer.b.shape[0]}x{layer.b.shape[1]}: {bias_text}\")\n",
        "        dot.node(f\"Layer {i+1} output\", f\"Layer {i+1} output ({output_size})\\nActivation: {activation_name}\\ny({i+1})={Sigmoid.sigma()}\", fontname=\"Arial\")\n",
        "\n",
        "        # Conectar la entrada con la salida de la misma capa\n",
        "        dot.edge(f\"Layer {i+1} input\", f\"Layer {i+1} output\", label=f\"Layer {i+1}\", fontname=\"Arial\", fontsize=\"10\", style=\"dotted\")\n",
        "\n",
        "        # Conectar la salida de la capa anterior con la entrada de la actual\n",
        "        if i > 0:\n",
        "            dot.edge(f\"Layer {i} output\", f\"Layer {i+1} input\", fontname=\"Arial\")\n",
        "        else:\n",
        "            # Conectar el nodo de entrada X con la primera capa\n",
        "            dot.edge(\"X\", f\"Layer {i+1} input\", fontname=\"Arial\")\n",
        "\n",
        "    # Crear el nodo de salida y_hat para la predicción\n",
        "    dot.node(\"y_hat\", \"Predicted Output y_hat\", fontname=\"Arial\")\n",
        "    dot.edge(f\"Layer {len(mlp.layers)} output\", \"y_hat\", fontname=\"Arial\")\n",
        "\n",
        "    # Crear el nodo de salida verdadera y\n",
        "    dot.node(\"y_true\", \"True Output y\", fontname=\"Arial\")\n",
        "\n",
        "    # Crear el nodo de la función de pérdida que conecta y_hat y y_true\n",
        "    dot.node(\"Loss\", \"Loss Function\", fontname=\"Arial\")\n",
        "    dot.edge(\"y_hat\", \"Loss\", label=\"y_hat\", fontname=\"Arial\")\n",
        "    dot.edge(\"y_true\", \"Loss\", label=\"y_true\", fontname=\"Arial\")\n",
        "\n",
        "    return dot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/svg+xml": [
              "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
              "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
              " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
              "<!-- Generated by graphviz version 12.1.2 (20240928.0832)\n",
              " -->\n",
              "<!-- Pages: 1 -->\n",
              "<svg width=\"1537pt\" height=\"242pt\"\n",
              " viewBox=\"0.00 0.00 1536.71 242.05\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
              "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 238.05)\">\n",
              "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-238.05 1532.71,-238.05 1532.71,4 -4,4\"/>\n",
              "<!-- X -->\n",
              "<g id=\"node1\" class=\"node\">\n",
              "<title>X</title>\n",
              "<ellipse fill=\"none\" stroke=\"black\" cx=\"39.09\" cy=\"-117.03\" rx=\"39.09\" ry=\"18\"/>\n",
              "<text text-anchor=\"middle\" x=\"39.09\" y=\"-111.6\" font-family=\"Arial\" font-size=\"14.00\">Input X</text>\n",
              "</g>\n",
              "<!-- Layer 1 input -->\n",
              "<g id=\"node2\" class=\"node\">\n",
              "<title>Layer 1 input</title>\n",
              "<ellipse fill=\"none\" stroke=\"black\" cx=\"195.96\" cy=\"-117.03\" rx=\"80.79\" ry=\"117.03\"/>\n",
              "<text text-anchor=\"middle\" x=\"195.96\" y=\"-182.48\" font-family=\"Arial\" font-size=\"14.00\">Layer 1 input (2)</text>\n",
              "<text text-anchor=\"middle\" x=\"195.96\" y=\"-166.73\" font-family=\"Arial\" font-size=\"14.00\">z(1)=W*X+b</text>\n",
              "<text text-anchor=\"middle\" x=\"195.96\" y=\"-150.98\" font-family=\"Arial\" font-size=\"14.00\">weights 3x2: </text>\n",
              "<text text-anchor=\"middle\" x=\"195.96\" y=\"-135.23\" font-family=\"Arial\" font-size=\"14.00\">[0.10, &#45;0.50</text>\n",
              "<text text-anchor=\"middle\" x=\"195.96\" y=\"-119.48\" font-family=\"Arial\" font-size=\"14.00\">&#45;0.30, &#45;0.90</text>\n",
              "<text text-anchor=\"middle\" x=\"195.96\" y=\"-103.73\" font-family=\"Arial\" font-size=\"14.00\">0.80, 0.02]</text>\n",
              "<text text-anchor=\"middle\" x=\"195.96\" y=\"-87.98\" font-family=\"Arial\" font-size=\"14.00\">bias 3x1: </text>\n",
              "<text text-anchor=\"middle\" x=\"195.96\" y=\"-72.23\" font-family=\"Arial\" font-size=\"14.00\">[0.10</text>\n",
              "<text text-anchor=\"middle\" x=\"195.96\" y=\"-56.48\" font-family=\"Arial\" font-size=\"14.00\">0.50</text>\n",
              "<text text-anchor=\"middle\" x=\"195.96\" y=\"-40.73\" font-family=\"Arial\" font-size=\"14.00\">0.80]</text>\n",
              "</g>\n",
              "<!-- X&#45;&gt;Layer 1 input -->\n",
              "<g id=\"edge2\" class=\"edge\">\n",
              "<title>X&#45;&gt;Layer 1 input</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M78.37,-117.03C86.09,-117.03 94.52,-117.03 103.2,-117.03\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"103.2,-120.53 113.2,-117.03 103.2,-113.53 103.2,-120.53\"/>\n",
              "</g>\n",
              "<!-- Layer 1 output -->\n",
              "<g id=\"node3\" class=\"node\">\n",
              "<title>Layer 1 output</title>\n",
              "<ellipse fill=\"none\" stroke=\"black\" cx=\"457.61\" cy=\"-117.03\" rx=\"112.61\" ry=\"39.07\"/>\n",
              "<text text-anchor=\"middle\" x=\"457.61\" y=\"-127.35\" font-family=\"Arial\" font-size=\"14.00\">Layer 1 output (3)</text>\n",
              "<text text-anchor=\"middle\" x=\"457.61\" y=\"-111.6\" font-family=\"Arial\" font-size=\"14.00\">Activation: Sigmoid</text>\n",
              "<text text-anchor=\"middle\" x=\"457.61\" y=\"-95.85\" font-family=\"Arial\" font-size=\"14.00\">y(1)=1 / (1 + np.exp(&#45;z))</text>\n",
              "</g>\n",
              "<!-- Layer 1 input&#45;&gt;Layer 1 output -->\n",
              "<g id=\"edge1\" class=\"edge\">\n",
              "<title>Layer 1 input&#45;&gt;Layer 1 output</title>\n",
              "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"1,5\" d=\"M276.98,-117.03C294.92,-117.03 314.35,-117.03 333.58,-117.03\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"333.36,-120.53 343.36,-117.03 333.36,-113.53 333.36,-120.53\"/>\n",
              "<text text-anchor=\"middle\" x=\"310.87\" y=\"-119.53\" font-family=\"Arial\" font-size=\"10.00\">Layer 1</text>\n",
              "</g>\n",
              "<!-- Layer 2 input -->\n",
              "<g id=\"node4\" class=\"node\">\n",
              "<title>Layer 2 input</title>\n",
              "<ellipse fill=\"none\" stroke=\"black\" cx=\"696.49\" cy=\"-117.03\" rx=\"89.27\" ry=\"72.48\"/>\n",
              "<text text-anchor=\"middle\" x=\"696.49\" y=\"-150.98\" font-family=\"Arial\" font-size=\"14.00\">Layer 2 input (3)</text>\n",
              "<text text-anchor=\"middle\" x=\"696.49\" y=\"-135.23\" font-family=\"Arial\" font-size=\"14.00\">z(2)=W*X+b</text>\n",
              "<text text-anchor=\"middle\" x=\"696.49\" y=\"-119.48\" font-family=\"Arial\" font-size=\"14.00\">weights 1x3: </text>\n",
              "<text text-anchor=\"middle\" x=\"696.49\" y=\"-103.73\" font-family=\"Arial\" font-size=\"14.00\">[&#45;0.40, 0.20, &#45;0.50]</text>\n",
              "<text text-anchor=\"middle\" x=\"696.49\" y=\"-87.98\" font-family=\"Arial\" font-size=\"14.00\">bias 1x1: </text>\n",
              "<text text-anchor=\"middle\" x=\"696.49\" y=\"-72.23\" font-family=\"Arial\" font-size=\"14.00\">[0.70]</text>\n",
              "</g>\n",
              "<!-- Layer 1 output&#45;&gt;Layer 2 input -->\n",
              "<g id=\"edge4\" class=\"edge\">\n",
              "<title>Layer 1 output&#45;&gt;Layer 2 input</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M570.46,-117.03C578.86,-117.03 587.3,-117.03 595.62,-117.03\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"595.36,-120.53 605.36,-117.03 595.36,-113.53 595.36,-120.53\"/>\n",
              "</g>\n",
              "<!-- Layer 2 output -->\n",
              "<g id=\"node5\" class=\"node\">\n",
              "<title>Layer 2 output</title>\n",
              "<ellipse fill=\"none\" stroke=\"black\" cx=\"966.61\" cy=\"-117.03\" rx=\"112.61\" ry=\"39.07\"/>\n",
              "<text text-anchor=\"middle\" x=\"966.61\" y=\"-127.35\" font-family=\"Arial\" font-size=\"14.00\">Layer 2 output (1)</text>\n",
              "<text text-anchor=\"middle\" x=\"966.61\" y=\"-111.6\" font-family=\"Arial\" font-size=\"14.00\">Activation: Sigmoid</text>\n",
              "<text text-anchor=\"middle\" x=\"966.61\" y=\"-95.85\" font-family=\"Arial\" font-size=\"14.00\">y(2)=1 / (1 + np.exp(&#45;z))</text>\n",
              "</g>\n",
              "<!-- Layer 2 input&#45;&gt;Layer 2 output -->\n",
              "<g id=\"edge3\" class=\"edge\">\n",
              "<title>Layer 2 input&#45;&gt;Layer 2 output</title>\n",
              "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"1,5\" d=\"M786.02,-117.03C804.05,-117.03 823.31,-117.03 842.29,-117.03\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"842.23,-120.53 852.23,-117.03 842.23,-113.53 842.23,-120.53\"/>\n",
              "<text text-anchor=\"middle\" x=\"819.88\" y=\"-119.53\" font-family=\"Arial\" font-size=\"10.00\">Layer 2</text>\n",
              "</g>\n",
              "<!-- y_hat -->\n",
              "<g id=\"node6\" class=\"node\">\n",
              "<title>y_hat</title>\n",
              "<ellipse fill=\"none\" stroke=\"black\" cx=\"1219.18\" cy=\"-117.03\" rx=\"102.96\" ry=\"18\"/>\n",
              "<text text-anchor=\"middle\" x=\"1219.18\" y=\"-111.6\" font-family=\"Arial\" font-size=\"14.00\">Predicted Output y_hat</text>\n",
              "</g>\n",
              "<!-- Layer 2 output&#45;&gt;y_hat -->\n",
              "<g id=\"edge5\" class=\"edge\">\n",
              "<title>Layer 2 output&#45;&gt;y_hat</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M1079.71,-117.03C1088.04,-117.03 1096.45,-117.03 1104.79,-117.03\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"1104.59,-120.53 1114.59,-117.03 1104.59,-113.53 1104.59,-120.53\"/>\n",
              "</g>\n",
              "<!-- Loss -->\n",
              "<g id=\"node8\" class=\"node\">\n",
              "<title>Loss</title>\n",
              "<ellipse fill=\"none\" stroke=\"black\" cx=\"1462.17\" cy=\"-82.03\" rx=\"66.53\" ry=\"18\"/>\n",
              "<text text-anchor=\"middle\" x=\"1462.17\" y=\"-76.6\" font-family=\"Arial\" font-size=\"14.00\">Loss Function</text>\n",
              "</g>\n",
              "<!-- y_hat&#45;&gt;Loss -->\n",
              "<g id=\"edge6\" class=\"edge\">\n",
              "<title>y_hat&#45;&gt;Loss</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M1299.76,-105.47C1329.41,-101.16 1362.82,-96.31 1391.51,-92.14\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"1391.78,-95.64 1401.17,-90.74 1390.77,-88.71 1391.78,-95.64\"/>\n",
              "<text text-anchor=\"middle\" x=\"1358.89\" y=\"-101.98\" font-family=\"Arial\" font-size=\"14.00\">y_hat</text>\n",
              "</g>\n",
              "<!-- y_true -->\n",
              "<g id=\"node7\" class=\"node\">\n",
              "<title>y_true</title>\n",
              "<ellipse fill=\"none\" stroke=\"black\" cx=\"1219.18\" cy=\"-63.03\" rx=\"66.03\" ry=\"18\"/>\n",
              "<text text-anchor=\"middle\" x=\"1219.18\" y=\"-57.6\" font-family=\"Arial\" font-size=\"14.00\">True Output y</text>\n",
              "</g>\n",
              "<!-- y_true&#45;&gt;Loss -->\n",
              "<g id=\"edge7\" class=\"edge\">\n",
              "<title>y_true&#45;&gt;Loss</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M1284.99,-65.32C1313.45,-66.61 1347.24,-68.53 1377.64,-71.28 1382.02,-71.67 1386.52,-72.12 1391.06,-72.61\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"1390.53,-76.07 1400.86,-73.72 1391.32,-69.12 1390.53,-76.07\"/>\n",
              "<text text-anchor=\"middle\" x=\"1358.89\" y=\"-73.73\" font-family=\"Arial\" font-size=\"14.00\">y_true</text>\n",
              "</g>\n",
              "</g>\n",
              "</svg>\n"
            ],
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x1082b1580>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dot = generate_mlp_graph(mlp)\n",
        "#dot.render('mlp_graph', format='png', view=True) \n",
        "display(dot)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "amia-j4FGlwjZ-py3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
