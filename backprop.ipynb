{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpOoxE2d-mOY"
      },
      "source": [
        "## Ejercicio teórico\n",
        "\n",
        "Sea una red neuronal de dos capas, la primera de 3 neuronas y la segunda de 1 con los parámetros inicializados con los siguientes valores:\n",
        "$$\n",
        "w^{(1)} =\n",
        "\\begin{pmatrix}\n",
        "0.1 & -0.5 \\\\\n",
        "-0.3 & -0.9 \\\\\n",
        "0.8 & 0.02\n",
        "\\end{pmatrix},\n",
        "b^{(1)} = \\begin{pmatrix}\n",
        "0.1 \\\\\n",
        "0.5 \\\\\n",
        "0.8\n",
        "\\end{pmatrix},\n",
        "w^{(2)} =\n",
        "\\begin{pmatrix}\n",
        "-0.4 & 0.2 & -0.5\n",
        "\\end{pmatrix},\n",
        "b^{(2)} = 0.7\n",
        "$$\n",
        "\n",
        "y donde cada capa calcula su salida vía\n",
        "\n",
        "$$\n",
        "y^{(i)} = \\sigma (w^{(i)} \\cdot x^{(i)}+b^{(i)})\n",
        "$$\n",
        "\n",
        "donde $\\sigma (z) = \\frac{1}{1+e^{-z}}$ es la función sigmoidea .\n",
        "\n",
        "\\\\\n",
        "Dada la observación $x=\\begin{pmatrix}\n",
        "1.8 \\\\\n",
        "-3.4\n",
        "\\end{pmatrix}$, $y=5$ y la función de costo $J(\\theta)=\\frac{1}{2}(\\hat{y}_\\theta-y)^2$, calcular las derivadas de J respecto de cada parámetro $w^{(1)}$, $w^{(2)}$, $b^{(1)}$, $b^{(2)}$.\n",
        "\n",
        "*Nota: Con una sigmoidea a la salida jamás va a poder estimar el 5 \"pedido\", pero eso no afecta al mecanismo de backpropagation!*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Layer(object):\n",
        "  def __init__(self, n_in, n_out, non_linearity_class, optimizer_factory, rng, w_init=None, b_init=None):\n",
        "    self.activation = non_linearity_class()\n",
        "    self.optim = optimizer_factory()\n",
        "    #self.w = rng.standard_normal(size=(n_out, n_in))  * 0.1 # W shape is (n_out,n_in)\n",
        "    #self.b = rng.uniform(size=(n_out, 1))                   # b shape is (n_out, 1)\n",
        "    # Valores de peso y bias precalculados\n",
        "    self.w = w_init if w_init is not None else rng.standard_normal(size=(n_out, n_in)) * 0.1\n",
        "    self.b = b_init if b_init is not None else rng.uniform(size=(n_out, 1))\n",
        "    self.last_output = None\n",
        "    self.last_input = None\n",
        "\n",
        "  def forward(self, X):\n",
        "    self.last_input = X\n",
        "    z = self.w @ X + self.b\n",
        "    self.last_output = self.activation.f(z)\n",
        "    print(\"--------------------------\")\n",
        "    print(\"Forward por capa z:\")\n",
        "    print(z)\n",
        "    print(\"Despues de la f de activación:\")\n",
        "    print(self.last_output)\n",
        "    print(\"--------------------------\")\n",
        "    return self.last_output\n",
        "\n",
        "  def backwards(self, dY):\n",
        "    dz = dY * self.activation.df()\n",
        "    dW = dz @ self.last_input.T\n",
        "    db = np.sum(dz, axis=1, keepdims=True)\n",
        "    dX = self.w.T @ dz\n",
        "    self.w, self.b = self.optim.update(self.w, self.b, dW, db)\n",
        "\n",
        "    # Mostrar derivadas parciales\n",
        "    print(\"--------------------------\")\n",
        "    print(\"Derivadas parciales respecto a W:\")\n",
        "    print(dW)\n",
        "    print(\"Derivadas parciales respecto a b:\")\n",
        "    print(\"--------------------------\")\n",
        "    print(db)\n",
        "    return dX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP(object):\n",
        "  def __init__(self, dims, optimizer_factory, non_linearities, input_dim, rng_seed = None, precalc_weights=None, precalc_biases=None):\n",
        "    # check lengths\n",
        "    if len(dims) != len(non_linearities):\n",
        "      raise ValueError(\"dims' and Non_linearities' lengths do not match\")\n",
        "    # initialize RNG\n",
        "    rng = np.random.default_rng(rng_seed)\n",
        "    # construct a list of Layers with matching dimension and non-linear activation function\n",
        "    in_dims = [input_dim] + dims[:-1]\n",
        "    #self.layers = [Layer(n_in, n_out, non_linearity, optimizer_factory, rng)\n",
        "    #                for n_in,n_out,non_linearity in zip(in_dims,dims, non_linearities)]\n",
        "    self.layers = [Layer(n_in, n_out, non_linearity, optimizer_factory, rng, w_init, b_init)\n",
        "                    for n_in, n_out, non_linearity, w_init, b_init \n",
        "                    in zip(in_dims, dims, non_linearities, precalc_weights, precalc_biases)]\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "    # X can be interpreted as the output of a previous layer\n",
        "    prediction = X\n",
        "    # sequentially apply forward pass\n",
        "    for layer in self.layers:\n",
        "      prediction = layer.forward(prediction)\n",
        "    return prediction\n",
        "\n",
        "  def update(self, cost_gradient):\n",
        "    # cost gradient is the cost derivative wrt last layer\n",
        "    dY = cost_gradient\n",
        "    # sequentially apply backwards update, in reversed order\n",
        "    for layer in reversed(self.layers):\n",
        "      dY = layer.backwards(dY)\n",
        "\n",
        "  def __repr__(self):\n",
        "    # super hardcoded\n",
        "    return \"MLP with layer sizes: \"+ \"-\".join(str(layer.b.shape[0]) for layer in self.layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Optimizer(object):\n",
        "  def update(self, W, b, dW, db):\n",
        "    raise NotImplementedError(\"optimizer update rule not implemented\")\n",
        "\n",
        "class VGD(Optimizer):\n",
        "  def __init__(self, learning_rate):\n",
        "    self.lr = learning_rate\n",
        "    \n",
        "  def update(self, W_old, b_old, dW, db):\n",
        "    # vanilla GD: theta_t+1 = theta_t - alpha * gradient\n",
        "    W_new = W_old - self.lr * dW\n",
        "    b_new = b_old - self.lr * db\n",
        "    return W_new, b_new\n",
        "\n",
        "def factory_VGD(lr):\n",
        "  return lambda : VGD(lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NonLinearity(object):\n",
        "  def __init__(self):\n",
        "    self.last_z = None\n",
        "  def f(self, z):\n",
        "    raise NotImplementedError(\"function evaluation not implemented\")\n",
        "  def df(self):\n",
        "    raise NotImplementedError(\"function derivative not implemented\")\n",
        "  \n",
        "class Sigmoid(NonLinearity):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def f(self, z):\n",
        "    self.last_z = z\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "  def df(self):\n",
        "    return np.exp(-self.last_z) / (1 + np.exp(-self.last_z))**2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Forward de entrada a dalida\n",
            "--------------------------\n",
            "Forward por capa z:\n",
            "[[1.98 ]\n",
            " [3.02 ]\n",
            " [2.172]]\n",
            "Despues de la f de activación:\n",
            "[[0.87868116]\n",
            " [0.95346953]\n",
            " [0.89770677]]\n",
            "--------------------------\n",
            "--------------------------\n",
            "Forward por capa z:\n",
            "[[0.09036805]]\n",
            "Despues de la f de activación:\n",
            "[[0.52257665]]\n",
            "--------------------------\n",
            "--------------------------\n",
            "Salida:  [[0.52257665]]\n",
            "--------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "lr = 0.001\n",
        "rng_seed = 6543\n",
        "\n",
        "# Pesos y sesgos precalculados para cada capa\n",
        "precalc_weights = [\n",
        "    np.array([[0.1, -0.5], [-0.3, -0.9], [0.8, 0.02]]),  # Pesos para la primera capa (3x2)\n",
        "    np.array([[-0.4, 0.2, -0.5]])                        # Pesos para la segunda capa (1x3)\n",
        "]\n",
        "precalc_biases = [\n",
        "    np.array([[0.1], [0.5], [0.8]]),  # Bias para la primera capa (3x1)\n",
        "    np.array([[0.7]])                 # Bias para la segunda capa (1x1)\n",
        "]\n",
        "\n",
        "# Configuración del MLP:\n",
        "dims = [3, 1]  # 3 neuronas en la primera capa, 1 neurona en la segunda\n",
        "input_dim = 2  # 2 entradas (dimensión de entrada)\n",
        "optimizer_factory = lambda: VGD(learning_rate=lr) #No se usa porque el modelo esta pre entrenado\n",
        "non_linearities = [Sigmoid, Sigmoid]  # Activaciones sigmoides para ambas capas\n",
        "\n",
        "# Crear la MLP con pesos y sesgos precalculados\n",
        "mlp = MLP(dims, optimizer_factory, non_linearities, input_dim, precalc_weights=precalc_weights, precalc_biases=precalc_biases)\n",
        "\n",
        "# Input de ejemplo (2 características de entrada)\n",
        "X = np.array([[1.8], [-3.4]])  # Tamaño del input (2x1)\n",
        "\n",
        "# Predecir con el MLP\n",
        "print(\"Forward de entrada a dalida\")\n",
        "output = mlp.predict(X)\n",
        "print(\"--------------------------\")\n",
        "print(\"Salida: \",output)\n",
        "print(\"--------------------------\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backprop de salida a entrada\n",
            "Error cuadratico medio:  [[10.02365992]]\n",
            "--------------------------\n",
            "Derivadas parciales respecto a W:\n",
            "[[-0.98155159 -1.0650957  -1.0028046 ]]\n",
            "Derivadas parciales respecto a b:\n",
            "--------------------------\n",
            "[[-1.11707367]]\n",
            "--------------------------\n",
            "Derivadas parciales respecto a W:\n",
            "[[ 0.0857381  -0.16194975]\n",
            " [-0.01784139  0.0337004 ]\n",
            " [ 0.09232211 -0.1743862 ]]\n",
            "Derivadas parciales respecto a b:\n",
            "--------------------------\n",
            "[[ 0.04763228]\n",
            " [-0.00991188]\n",
            " [ 0.05129006]]\n"
          ]
        }
      ],
      "source": [
        "print(\"Backprop de salida a entrada\")\n",
        "y_true = 5\n",
        "error = 1/2 * (output - y_true)**2\n",
        "print(\"Error cuadratico medio: \", error)\n",
        "mlp.update(output - y_true)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "amia-j4FGlwjZ-py3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
